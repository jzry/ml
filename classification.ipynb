{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "This project asks you to perform various experiments with classification. The dataset we are using is a toy dataset for credit card fraud detection:\n",
    "\n",
    "https://www.kaggle.com/datasets/shubhamjoshi2130of/abstract-data-set-for-credit-card-fraud-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for the project\n",
    "\n",
    "Here we load the dataset, and create the training and test datasets as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 3075\n",
      "The columns of the database Index(['Merchant_id', 'Transaction date', 'Average Amount/transaction/day',\n",
      "       'Transaction_amount', 'Is declined', 'Total Number of declines/day',\n",
      "       'isForeignTransaction', 'isHighRiskCountry', 'Daily_chargeback_avg_amt',\n",
      "       '6_month_avg_chbk_amt', '6-month_chbk_freq', 'isFradulent'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isFradulent\n",
       "False    2627\n",
       "True      448\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\",  true_values=\"Y\", false_values=\"N\")\n",
    "print(f\"Number of rows {len(df.index)}\")\n",
    "print(f\"The columns of the database {df.columns}\")\n",
    "df.value_counts(\"isFradulent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfields = [\n",
    "    'Average Amount/transaction/day',\n",
    "       'Transaction_amount', 'Is declined', 'Total Number of declines/day',\n",
    "       'isForeignTransaction', 'isHighRiskCountry', 'Daily_chargeback_avg_amt',\n",
    "       '6_month_avg_chbk_amt', '6-month_chbk_freq']\n",
    "\n",
    "df_shuffled = df.sample(frac=1) # shuffle the rows\n",
    "x = df_shuffled[xfields].to_numpy(dtype=np.float64)\n",
    "y = df_shuffled[\"isFradulent\"].to_numpy(dtype=np.float64)\n",
    "# the training data is the first 2000 rows, after shuffled\n",
    "training_data_x = x[:2000]\n",
    "training_data_y = y[:2000]\n",
    "# the test data is the remaining\n",
    "test_data_x = x[2000:]\n",
    "test_data_y = y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this to help you with what number goes with what field:\n",
      "0 = Average Amount/transaction/day\n",
      "1 = Transaction_amount\n",
      "2 = Is declined\n",
      "3 = Total Number of declines/day\n",
      "4 = isForeignTransaction\n",
      "5 = isHighRiskCountry\n",
      "6 = Daily_chargeback_avg_amt\n",
      "7 = 6_month_avg_chbk_amt\n",
      "8 = 6-month_chbk_freq\n"
     ]
    }
   ],
   "source": [
    "print(\"Run this to help you with what number goes with what field:\")\n",
    "for i, x in enumerate(xfields):\n",
    "    print(f\"{i} = {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape\n",
      "(2000, 9)\n",
      "train_y.shape\n",
      "(2000,)\n",
      "test_x.shape\n",
      "(1075, 9)\n",
      "test_y.shape\n",
      "(1075,)\n"
     ]
    }
   ],
   "source": [
    "print(f'train_x.shape\\n{training_data_x.shape}')\n",
    "print(f'train_y.shape\\n{training_data_y.shape}')\n",
    "print(f'test_x.shape\\n{test_data_x.shape}')\n",
    "print(f'test_y.shape\\n{test_data_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: Create an accuracy metric (7 pts)\n",
    "Create a simple accuracy metric function which for a pair of ground truth values $y$ and estimates $\\hat{y}$ (both of them arrays) calculates the accuracy of the estimate $\\hat{y}$. For instance, if you pass y = [1, 0, 1] and \n",
    "yhat = [1, 1, 0], the loss function should return 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns (number of correct predictions / total number of predictions).\n",
    "# Each prediction value should match with the accurate data value.\n",
    "# y is the accurate data yhat is the prediction.\n",
    "def accuracy(y, yhat):\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    num_correct_pred = 0\n",
    "    total_num_pred = len(y)\n",
    "    trials = len(y)\n",
    "    \n",
    "    for i in range(trials):\n",
    "        if yhat[i] == y[i]:\n",
    "            num_correct_pred += 1\n",
    "\n",
    "    accuracy = (num_correct_pred / total_num_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.3333333333333333\n",
      "Accuracy is 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# test your function here\n",
    "acc = accuracy([1, 0, 1], [1, 1, 0])\n",
    "print(f\"Accuracy is {acc}\") # should print 0.33...\n",
    "acc = accuracy([1, 0, 1, 0, 1, 1], [0, 0, 0, 0, 1, 1])\n",
    "print(f\"Accuracy is {acc}\") # should print 0.66..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2: Implement a majority classifier (7 pts)\n",
    "This classifier will always return the most likely value. Training the classifier means determining what is the most likely value (regardless vhat value you pass to it). For instance, if more than half of the transactions are fraudulent, then you just return fraudulent always. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a 2D array of new transaction data.\n",
    "# Averages is a 2D array of the averages of columns for fraud and non-fraud data.\n",
    "# row 0 contains averages of columns for fraud.\n",
    "# row 1 contains averages of columns for non-fraud.\n",
    "# Returns a string as either \"Fraud\" or \"Non-fraud\". This can be changed to bool values.\n",
    "def classify_majority(x, averages):\n",
    "    \n",
    "    # When I state \"row\" here, I may be referring to transaction row, or x[row].\n",
    "    # Every x[row] is a transaction's details, which we use for classification.\n",
    "    \n",
    "    # Get the number of rows and columns for the input parameters.\n",
    "    x_rows = x.shape[0]\n",
    "    x_cols = x.shape[1]\n",
    "        \n",
    "    # Contains the return classification array for every transaction.\n",
    "    # 0 represents no fraud, 1 represents fraud.\n",
    "    predictions = np.zeros(x_rows)\n",
    "\n",
    "    # Contains the intermediate 0,1 array to calculate majority.\n",
    "    classifications = np.zeros((x_rows, x_cols))\n",
    "\n",
    "    for i in range(x_rows):\n",
    "        for j in range(x_cols):\n",
    "            \n",
    "            # Find difference for every value in x row compared to average of F / NF.\n",
    "            nonfraud_diff = abs(x[i][j] - averages[0][j]) # Nonfraud\n",
    "            fraud_diff = abs(x[i][j] - averages[1][j]) # Fraud\n",
    "            \n",
    "            # X[i][j] value is closer to fraud average than nonfraud.\n",
    "            if fraud_diff <= nonfraud_diff:\n",
    "                classifications[i][j] = 1 # Classify as fraud\n",
    "            else:\n",
    "                classifications[i][j] = 0 # Classify as nonfraud\n",
    "        \n",
    "        # Classification array contains only 0's, 1's. Same shape as training_x data.\n",
    "        # Find the majority values (whether F / NF) for every transaction (row).\n",
    "        # If 30%+ of transaction's features are more similar to fraud, label transaction F.\n",
    "        if ((np.sum(classifications[i]) / x_cols) >= 0.30):\n",
    "            predictions[i] = 1 # Fraud\n",
    "        else:\n",
    "            predictions[i] = 0 # Nonfraud\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Train_x contains a 2D array that contains 8 features and a large number of rows of data.\n",
    "# Train_y contains a 1D array that classifies the transaction as fraudulent or not.\n",
    "# Given training data, return a 2D called averages that is described as follows:\n",
    "# 1st row contains the averages of every column for the fraud data.\n",
    "# 2nd row contains the averages of every column for the non-fraud data.\n",
    "# Using the averages of the data determining whether they are fraud/nonfraud, we can\n",
    "# determine the likelihood that the data is either fraud/nonfraud and classify it.\n",
    "def train_majority(training_x, training_y):\n",
    "    \n",
    "    # Extract row and column data from training data.\n",
    "    rows = training_x.shape[0] # rows\n",
    "    cols = training_x.shape[1] # cols\n",
    "    \n",
    "    # Make a 2d array.\n",
    "    # 1st row (0) stores averages of non-fraud, 2nd row (1) stores fraud averages.\n",
    "    averages = np.zeros(shape=(2, cols))\n",
    "\n",
    "    # Extract all fraud and not fraud value rows to mask.\n",
    "    fraud_mask = np.where(training_y == 1.0)\n",
    "    non_fraud_mask = np.where(training_y == 0.0)\n",
    "    \n",
    "    # Get fraud and non fraud rows.\n",
    "    fraud = training_x[fraud_mask]\n",
    "    nonfraud = training_x[non_fraud_mask]\n",
    "\n",
    "    # print(\"average of first row (doesn't make sense)\", np.mean(fraud[0]))\n",
    "    # print(\"average of first column\", np.mean(fraud[:,0]))\n",
    "    \n",
    "    # print(\"averages\", averages.shape)\n",
    "    # print(\"fraud\", fraud.shape)\n",
    "    # print(\"nonfraud\", nonfraud.shape)\n",
    "    \n",
    "    # This gives an array of the first column for fraud.\n",
    "    # print(\"fraud[:,0]\", fraud[:,0])\n",
    "    # This gives an array of the first row for fraud.\n",
    "    # print(\"fraud[0]\", fraud[0])\n",
    "    \n",
    "    # Get averages for each column in fraud.\n",
    "    for i in range(averages.shape[0]): # Loop through rows\n",
    "        for j in range(averages.shape[1]): # Loop through columns\n",
    "                \n",
    "            # Extract the mean from every column for nonfraud average.\n",
    "            if i == 0:\n",
    "                averages[i][j] = np.mean(nonfraud[:,j])\n",
    "                \n",
    "            # Extract the mean from every column for fraud average.\n",
    "            if i == 1:\n",
    "                averages[i][j] = np.mean(fraud[:,j])\n",
    "                \n",
    "    # print(\"averages of nonfraud\", averages[0])\n",
    "    # print(\"averages of fraud\", averages[1])\n",
    "    \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-675101f43c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Zipping up the total counts of unique fraud / nonfraud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Predictions to visualize the unique value and counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtest_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mrealTrain_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealTrain_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_pred' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# TODO: use the train_majority function to find the theta value for the training dataset\n",
    "\n",
    "# Returns a 2D array of the averages for the train/test data fraud and non-fraud.\n",
    "# train_averages[0] = nonfraud averages, train_averages[1] = fraud averages\n",
    "train_start = time.time()\n",
    "train_averages = train_majority(training_data_x, training_data_y)\n",
    "train_end = time.time()\n",
    "test_averages = train_majority(test_data_x, test_data_y)\n",
    "\n",
    "# TODO: now use the theta value to create the test_data_yhat array which contains the classification for each test value \n",
    "label_start = time.time()\n",
    "train_pred_using_train_avg = classify_majority(training_data_x, train_averages)\n",
    "label_end = time.time()\n",
    "test_pred_using_train_avg = classify_majority(test_data_x, train_averages)\n",
    "train_pred_using_test_avg = classify_majority(training_data_x, test_averages)\n",
    "test_pred_using_test_avg = classify_majority(test_data_x, test_averages)\n",
    "\n",
    "# Zipping up the total counts of unique fraud / nonfraud\n",
    "# Predictions to visualize the unique value and counts.\n",
    "train_unique, train_counts = np.unique(train_pred, return_counts=True)\n",
    "test_unique, test_counts = np.unique(test_pred, return_counts=True)\n",
    "realTrain_unique, realTrain_counts = np.unique(training_data_y, return_counts=True)\n",
    "realTest_unique, realTest_counts = np.unique(test_data_y, return_counts=True)\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"0 = No fraud, 1 = Fraud\")\n",
    "print(\"==================================\")\n",
    "print(\"Training time: \", round(train_end - train_start, 4), \"seconds\")\n",
    "print(\"Labeling time: \", round(label_end - label_start, 4), \"seconds\")\n",
    "print(\"==================================\")\n",
    "print(\"averages of nonfraud averages using training\\n\", train_averages[0])\n",
    "print(\"averages of fraud averages using training data\\n\", train_averages[1])\n",
    "print(\"==================================\")\n",
    "print(\"Train predictions:\\n\", train_pred_using_train_avg[0:20])\n",
    "print(\"Train real:\\n\", training_data_y[0:20])\n",
    "print(\"Test predictions:\\n\", test_pred_using_train_avg[0:20])\n",
    "print(\"Test real:\\n\", test_data_y[0:20])\n",
    "print(\"==================================\")\n",
    "print(\"Train pred unique counts:\\n\", dict(zip(train_unique, train_counts)))\n",
    "print(\"Train real unique counts:\\n\", dict(zip(realTrain_unique, realTrain_counts)))\n",
    "print(\"Test pred unique counts:\\n\", dict(zip(test_unique, test_counts)))\n",
    "print(\"Test real unique counts:\\n\", dict(zip(realTest_unique, realTest_counts)))\n",
    "print(\"==================================\")\n",
    "\n",
    "# TODO: now calculate the accuracy of the classifier using the function implemented in P1, and print it out\n",
    "print(\"Train Fraud averages:\\n\", train_averages[0])\n",
    "print(\"Train Non-fraud averages:\\n\", train_averages[1])\n",
    "print(\"Test Fraud averages:\\n\", test_averages[0])\n",
    "print(\"Test Non-fraud averages:\\n\", test_averages[1])\n",
    "print(\"==================================\")\n",
    "print(f'TRAIN ACCURACY using training averages:\\t{accuracy(training_data_y, train_pred_using_train_avg)}')\n",
    "print(f'TEST ACCURACY using training averages:\\t{accuracy(test_data_y, test_pred_using_train_avg)}')\n",
    "print(f'TRAIN ACCURACY using testing averages:\\t{accuracy(training_data_y, train_pred_using_test_avg)}')\n",
    "print(f'TEST ACCURACY using testing averages:\\t{accuracy(test_data_y, test_pred_using_test_avg)}')\n",
    "print(\"==================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of the data values.\n",
    "averages = train_majority(training_data_x, training_data_y)\n",
    "\n",
    "print(\"averages of fraud\\n\", averages[0])\n",
    "print(\"averages of nonfraud\", averages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis and exploration for training data only\n",
    "\n",
    "## Comparison of fraud / non-fraud feature averages\n",
    "- Average Amount/transaction/day\n",
    "    + Fraud: 536\n",
    "    + Non-fraud: 509\n",
    "- Transaction_amount\n",
    "    + Fraud: 23,190\n",
    "    + Non-fraud: 7,704\n",
    "- Is declined\n",
    "    + Fraud: 0.1127 (11.27%)\n",
    "    + Non-fraud: 0.0023 (0.23%)\n",
    "- Total Number of declines/day\n",
    "    + Fraud: 3.8945\n",
    "    + Non-fraud: 0.4556\n",
    "- isForeignTransaction\n",
    "    + Fraud: 0.6981 (69.81%)\n",
    "    + Non-fraud: 0.1420 (14.20%)\n",
    "- isHighRiskCountry\n",
    "    + Fraud: 0.4581 (45.81%)\n",
    "    + Non-fraud: 0.0005 (0.05%)\n",
    "- Daily_chargeback_avg_amt\n",
    "    + Fraud: 268\n",
    "    + Non-fraud: 22\n",
    "- 6_month_avg_chbk_amt\n",
    "    + Fraud: 196\n",
    "    + Non-fraud: 15\n",
    "- 6-month_chbk_freq\n",
    "    + Fraud: 2.2218\n",
    "    + Non-fraud: 0.1008\n",
    "    \n",
    "## Conclusion from comparing fraud and non-fraud averages\n",
    "Fraud averages were significantly higher for every category in comparison to non-fraud averages. We can use this info to create an algorithmic flagging system for fraud detection.\n",
    "\n",
    "Assume we have a new transaction and need to determine whether the data is more similar to either fraud or non-fraud. We can do this by determining if new data has higher averages than fraud data.\n",
    "\n",
    "To determine similarity it would be suitable to compare the value of the new transaction columns to the averages of fraud and non-fraud and then determine which value the new transaction column data is closer to.\n",
    "\n",
    "Example psuedo-code algorithm:\n",
    "\n",
    "```\n",
    "def categorize(new_transaction, averages):\n",
    "\n",
    "# each index contains either a 0 or 1. 0 indicates that new transaction cell was less than\n",
    "# the average of fraud. 1 indicates that the new transaction cell was higher than the average.\n",
    "# We will take the sum of all categories and divide by the number of cateogires to determine\n",
    "# The percentage likelihood that the new transaction is either fraud or not fraud.\n",
    "fraud_points = np.zeros(len(new_transaction))\n",
    "\n",
    "for i in range(len(new_transaction.shape[0])): # Rows\n",
    "    for j in range(len(new_transaction.shape[1]): # Columns\n",
    "       if new_transaction[i] >= averages[i][j] # Fraud\n",
    "           category_points[i] += 1\n",
    "       \n",
    "# This is the classification percentage for the new transaction.\n",
    "category_percentage = np.sum(fraud_points) / len(fraud_points)\n",
    "\n",
    "# if category_percentage is greater than 50%, we are notified of fraud.\n",
    "if category_percentage > 0.50:\n",
    "    category = 'Fraud'\n",
    "else:\n",
    "    category = 'Non-fraud'\n",
    "\n",
    "return category\n",
    "```\n",
    "\n",
    "## Pseudocode explanation\n",
    "Fraud points is an array of zeros of shape (new transaction rows, new transaction columns). For every row, we are comparing the data in every column to the fraud columns (fraud averages). If the new transaction row contains data that is higher than the fraud average, we add a point to symbolize that is a flag that could later be used to categorize if a new transaction is fraudulent or not.\n",
    "\n",
    "- Example:\n",
    "```\n",
    "category_points = [0, 1, 0, 1, 0, 0] # size: 6\n",
    "category_percentage = 2 fraud points / size 6\n",
    "category_percentage = 0.33 # 33%\n",
    "```\n",
    "We can therefore determine that this transaction is not likely to be fraud.\n",
    "\n",
    "## Improvements\n",
    "- We are assuming that all categories in classifying fraud have equal weight, however the daily transaction amount for fraud is very similar to non-fraud.\n",
    "- It may be useful to incorporate mode and median values of the fraud and non-fraud training data into the algorithm to get more metrics to compare the new transaction data to.\n",
    "- I have created this with the assumption that new_transaction is a 1D array, not sure if the algorithm would change if it was a 2D array but probably.\n",
    "- The only metric we are using to adding a new point to the category is whether the new transaction data averages are greater than the fraud averages, which is very reductionist. There may be legitimate transactions that have greater values for fraud which may be flagged.\n",
    "- Category percentage value is hard coded to 50%. There may be a more optimal value to determine whether to be notified of fraud. It may be lower or higher than 50%.\n",
    "- If fraud and non-fraud averages are very similar, this algorithm may be useless and produce too many false positives. This would mean that fraud and non-fraud transactions share too many similarities and one can not determine whether there is a difference in the averages of fraud data and non-fraud data.\n",
    "\n",
    "## Major problem I ran into\n",
    "When I made the initial algorithm up above I assumed that the function for classification would receive training data x and training data y, however we can not pass it y because that would be cheating. Therefore, I had to think up a new solution that is able to classify data into fraud or non-fraud using the averages array parameter passed in. Here's what I came up with:\n",
    "\n",
    "```\n",
    "Given:\n",
    "x = [[n, n, n, ... n, n, n], [n, n, n, ... n, n, n], ... [n, n, n, ... n, n, n]]\n",
    "averages = [Fraud = [n, n, n, ... n, n, n], Nonfraud = [n, n, n, ... n, n, n]]\n",
    "\n",
    "See if x[row] is more similar to average row Fraud or average row Nonfraud.\n",
    "Similarity is based on the absolute value of x[row] - (comparing array list).\n",
    "\n",
    "# Initially all transactions are considered fraud until proven innocent.\n",
    "# 0 represents non fraud, 1 represents fraud.\n",
    "# 1D array the size of training x data columns to classify the transaction as F or NF.\n",
    "is_row_fraud = np.ones(train_x_rows)\n",
    "\n",
    "# Get the sum of both averages to use for comparison against the training x rows.\n",
    "fraud_sum = np.sum(averages[0])\n",
    "nonfraud_sum = np.sum(averages[1])\n",
    "\n",
    "# Go through every row and every column for training data x.\n",
    "for i in range(col):\n",
    "    for j in range(row):\n",
    "        # Get the sum for the row in the training data.\n",
    "        x_row_sum = np.sum(x[j])\n",
    "        \n",
    "        # Get the absolute value difference in sums from the x row and fraud / nonfraud.\n",
    "        fraud_diff = abs(x_row_sum - fraud_sum)\n",
    "        nonfraud_diff = abs(x_row_sum - nonfraud_sum)\n",
    "        \n",
    "        # Compare the differences to see whether x is closer to fraud / nonfraud.\n",
    "        # Categorize it by placing either a 1 for fraud, or a 0 for nonfraud in the\n",
    "        # is_row_fraud tracker.\n",
    "        if fraud_diff <= nonfraud_diff:\n",
    "            is_row_fraud[j] = 0\n",
    "```\n",
    "\n",
    "- When doing a for loop to compare np.mean(x[:,i]) > averages[0][i], we are accidentally comparing the average of both fraud and non-fraud at the same time. I would have to do np.where(x[:,np.where(y == 1)]) > averages[0][i]\n",
    "\n",
    "## After fixing major improvement log\n",
    "Currently I am doing this:\n",
    "\n",
    "```\n",
    "for i in range(x_cols):\n",
    "x_train_sum = np.sum(x[row])\n",
    "fraud_sum = np.sum(average[fraud_row])\n",
    "nonfraud_sum = np.sum(average[nonfraud_row])\n",
    "\n",
    "if abs(x_train_sum - fraud_sum) <= abs(x_train_sum - nonfraud_sum)\n",
    "    classification_predictions[row] = labelled_fraud\n",
    "```\n",
    "\n",
    "However there is a problem with this. We are assuming that the sum of the training data works as a whole to compare for every feature, but some features are only yes and no. Therefore, it may be more suitable to take the differences of each column and row, and then loop through the array of differences for fraud - x[row] and nonfraud - x[row] and a majority (50%+) of the differences are more similar to either fraud or nonfraud.\n",
    "\n",
    "Something like:\n",
    "```\n",
    "# Contains the return classification array for every transaction.\n",
    "# 0 represents no fraud, 1 represents fraud.\n",
    "predictions = np.zeros(x_rows)\n",
    "\n",
    "# Contains the intermediate 0,1 array to calculate majority.\n",
    "classifications = np.zeros(x_rows, x_cols)\n",
    "\n",
    "\n",
    "# Get the difference for ever F / NF value, compare every value in each index to check\n",
    "# Which classification the x index is closer to. If closer to fraud averages, classify\n",
    "# The index in that x index as a fraud similarity, if closer to NF, then NF.\n",
    "# We will have a classification 2D index identical in shape to x as so:\n",
    "# [[1, 0, 1, 1... 1, 0], ... [0, 1, 0, 1... 0, 1]].\n",
    "# Where 1 = fraud similarity, 0 = nonfraud similarity.\n",
    "# Now we loop through this array to find whether each row (transaction) contains more F/NF\n",
    "# similarity. If we have array = [1, 0, 1, 1, 0, 1], then 4 fraud instances / size 6 = .66%\n",
    "# So we can classify that transaction as fraud.\n",
    "# We would put that classification in a predictions 1D array of size columns.\n",
    "# predictions[i] = (np.sum(classifications[i]) / x_cols)\n",
    "# predictions = [1, 0, 1, ... 0, 1, 0] transaction[0] = fraud, [1] = nonfraud, etc.\n",
    "for i in range(x_rows):\n",
    "    for j in range(x_cols):\n",
    "    \n",
    "        fraud_diff = abs(x[i][j] - averages[i][j])\n",
    "        nonfraud_diff = abs(x[i][j] - averages[i][j])    \n",
    "        \n",
    "        if fraud_diff <= nonfraud_diff:\n",
    "            classifications[i][j] = 1\n",
    "        else:\n",
    "            classifications[i][j] = 0\n",
    "     \n",
    "    if ((np.sum(classification[i]) / x_cols) >= 0.50):\n",
    "        predictions[i] = 1 # Fraud\n",
    "    else:\n",
    "        predictions[i] = 0 # Nonfraud\n",
    "        \n",
    "return predictions\n",
    "        \n",
    "```\n",
    "        \n",
    "## Final judgement\n",
    "\n",
    "I have finally finished the final product and it works perfectly, although it could always use improvements.\n",
    "\n",
    "Here is a small sample of the data.\n",
    "### 0 = No fraud, 1 = Fraud\n",
    "- Train predictions:\n",
    "    + [0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
    "- Train real:\n",
    "    + [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
    "- Test predictions:\n",
    "    + [0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
    "- Test real:\n",
    "    + [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
    "- Train pred unique counts:\n",
    "    + {0.0: 1287, 1.0: 713}\n",
    "- Train real unique counts:\n",
    "    + {0.0: 1725, 1.0: 275}\n",
    "- Test pred unique counts:\n",
    "    + {0.0: 687, 1.0: 388}\n",
    "- Test real unique counts:\n",
    "    + {0.0: 902, 1.0: 173}\n",
    "- Fraud averages:\n",
    "    + [5.36797366e+02 2.31905750e+04 1.12727273e-01 3.89454545e+00\n",
    " 6.98181818e-01 4.58181818e-01 2.68025455e+02 1.96155273e+02\n",
    " 2.22181818e+00]\n",
    "- Non-fraud averages:\n",
    "    + [5.09289820e+02 7.70428074e+03 2.31884058e-03 4.55652174e-01\n",
    " 1.42028986e-01 5.79710145e-04 2.22985507e+01 1.49634783e+01\n",
    " 1.00869565e-01]\n",
    "- **Accuracy:**\n",
    "    + **0.8715**\n",
    "- **Accuracy:**\n",
    "    + **0.8623255813953489**\n",
    "\n",
    "OUR ACCURACY ~87% FOR THE TRAINING AND TESTING DATA!\n",
    "THAT IS INCREDIBLE FOR A TRAINING MODEL. MY MODEL IS THE BEST MODEL.\n",
    "\n",
    "## Final edit:\n",
    "I realized I was classifying using only the training data generate average values, but now I changed the code to also display the testing data generated values and found the optimal value of >30% for classifying a transaction as fraudulent. If 30% of the features are found to be similar to fraudulent transactions, we will mark that transaction of fradulent.\n",
    "\n",
    "My final accuracy is the following:\n",
    "\n",
    "- Train Accuracy using training averages:\t0.9225\n",
    "- Test Accuracy using training averages:\t0.9079069767441861\n",
    "- Train Accuracy using testing averages:\t0.922\n",
    "- Test Accuracy using testing averages:     0.9041860465116279\n",
    "\n",
    "# **On average, my model has a ~91% accuracy in predicting a fraudulent transaction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3: Implement a hand engineered classifier (8 pts)\n",
    "\n",
    "Engineer by hand a classifier function that predicts whether  a transaction is  fraudulent or not. Your function should have a $\\theta$ parameter which allows you to tweak it. \n",
    "The problem requires you to design a function that performs this classification, tweak its parameters, and measure its accuracy for the best parametrization you found. You should aim for a function that, at minimum, performs better than the majority classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation for my handwritten function\n",
    "\n",
    "We have the averages of the fraud and non-fraud columns.\n",
    "\n",
    "We will continue to use the averages of the fraud and nonfraud columns, but place more emphasis here on the whether a transaction is in a foreign country, high risk country, and chargeback and checkbook balance. These values contribute to a significantly higher increase in risk for a fraudulent transaction. Let's break down the data:\n",
    "\n",
    "## Comparison of fraud / non-fraud feature averages\n",
    "- Average Amount/transaction/day\n",
    "    + Fraud: 536\n",
    "    + Non-fraud: 509\n",
    "- Transaction_amount\n",
    "    + Fraud: 23,190\n",
    "    + Non-fraud: 7,704\n",
    "- Is declined\n",
    "    + Fraud: 0.1127 (11.27%)\n",
    "    + Non-fraud: 0.0023 (0.23%)\n",
    "- Total Number of declines/day\n",
    "    + Fraud: 3.8945\n",
    "    + Non-fraud: 0.4556\n",
    "- isForeignTransaction\n",
    "    + Fraud: 0.6981 (69.81%)\n",
    "    + Non-fraud: 0.1420 (14.20%)\n",
    "- isHighRiskCountry\n",
    "    + Fraud: 0.4581 (45.81%)\n",
    "    + Non-fraud: 0.0005 (0.05%)\n",
    "- Daily_chargeback_avg_amt\n",
    "    + Fraud: 268\n",
    "    + Non-fraud: 22\n",
    "- 6_month_avg_chbk_amt\n",
    "    + Fraud: 196\n",
    "    + Non-fraud: 15\n",
    "- 6-month_chbk_freq\n",
    "    + Fraud: 2.2218\n",
    "    + Non-fraud: 0.1008\n",
    "\n",
    "## Fraudulent transactions...\n",
    "- have slightly higher transaction counts\n",
    "- transact 3x the amount as non-fraud transactions\n",
    "- 49x more the amount of declines per day\n",
    "- 8.5x more the amount of declines\n",
    "- 5x more likely to be a foreign transaction\n",
    "- 916x more likely to be in a high risk country\n",
    "- 12x more likely to have higher chargeback average\n",
    "- 13x more likely to have a higher checkbook amount\n",
    "- 22x more likely to have a higher checkbook frequency\n",
    "\n",
    "## Conclusion\n",
    "The most starting numbers that stick out are:\n",
    "- 49x higher number of declines.\n",
    "- 916x the amount of fraud cases are in a high risk country.\n",
    "\n",
    "So our new handwritten function will take those two factors in a heavier weight than our automated identification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement here your hand-engineered classifier\n",
    "# The example below is just a very bad example, but it gives you an idea of how you can reason about the classification problem.\n",
    "# In your implementation, you should try to actually find some kind of clever algorithm. You can also use more complex parametrizations\n",
    "\n",
    "# Higher weighting is attributed towards isDeclined, isHighRiskCountry, and checkbook_freq.\n",
    "# Changed theta to averages. Same thing.\n",
    "def classify_handwritten(x, averages):\n",
    "    \n",
    "    # Get the number of rows and columns for the input parameters.\n",
    "    x_rows = x.shape[0]\n",
    "    x_cols = x.shape[1]\n",
    "        \n",
    "    # Contains the return classification array for every transaction.\n",
    "    # 0 represents no fraud, 1 represents fraud.\n",
    "    predictions = np.zeros(x_rows)\n",
    "\n",
    "    # Contains the intermediate 0,1 array to calculate majority.\n",
    "    classifications = np.zeros((x_rows, x_cols))\n",
    "\n",
    "    for i in range(x_rows):\n",
    "        for j in range(x_cols):\n",
    "            \n",
    "            # Find difference for every value in x row compared to average of F / NF.\n",
    "            nonfraud_diff = abs(x[i][j] - averages[0][j]) # Nonfraud\n",
    "            fraud_diff = abs(x[i][j] - averages[1][j]) # Fraud\n",
    "            \n",
    "            # X[i][j] value is closer to fraud average than nonfraud.\n",
    "            if fraud_diff <= nonfraud_diff:\n",
    "                classifications[i][j] = 1 # Classify as fraud\n",
    "            else:\n",
    "                classifications[i][j] = 0 # Classify as nonfraud\n",
    "        \n",
    "        # Classification array contains only 0's, 1's. Same shape as training_x data.\n",
    "        # Highest risk factors are isDeclined (2), isHighRiskCountry (5), and checkbook_freq (8).\n",
    "        highest_risk_factors = ((classifications[i][2] + classifications[i][5] + classifications[i][8]) / 3)\n",
    "        \n",
    "        if (highest_risk_factors >= 0.3) or ((np.sum(classifications[i]) / classifications.shape[1]) >= 0.3):\n",
    "            predictions[i] = 1 # Fraud\n",
    "        else:\n",
    "            predictions[i] = 0 # Nonfraud\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now, run some experiments with your function. Experiment with different values of the parameter theta.  \n",
    "\n",
    "import time\n",
    "\n",
    "# Returns a 2D array of the averages for the train/test data fraud and non-fraud.\n",
    "# train_averages[0] = nonfraud averages, train_averages[1] = fraud averages\n",
    "train_start = time.time()\n",
    "train_averages = train_majority(training_data_x, training_data_y)\n",
    "train_end = time.time()\n",
    "test_averages = train_majority(test_data_x, test_data_y)\n",
    "\n",
    "# TODO: now use the theta value to create the test_data_yhat array which contains the classification for each test value \n",
    "label_start = time.time()\n",
    "train_pred_using_train_avg = classify_handwritten(training_data_x, train_averages)\n",
    "label_end = time.time()\n",
    "test_pred_using_train_avg = classify_handwritten(test_data_x, train_averages)\n",
    "train_pred_using_test_avg = classify_handwritten(training_data_x, test_averages)\n",
    "test_pred_using_test_avg = classify_handwritten(test_data_x, test_averages)\n",
    "\n",
    "# Zipping up the total counts of unique fraud / nonfraud\n",
    "# Predictions to visualize the unique value and counts.\n",
    "train_unique, train_counts = np.unique(train_pred, return_counts=True)\n",
    "test_unique, test_counts = np.unique(test_pred, return_counts=True)\n",
    "realTrain_unique, realTrain_counts = np.unique(training_data_y, return_counts=True)\n",
    "realTest_unique, realTest_counts = np.unique(test_data_y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the accuracy of the classifier on the test data with the best\n",
    "# theta found above and print it.\n",
    "print(\"==================================\")\n",
    "print(\"0 = No fraud, 1 = Fraud\")\n",
    "print(\"==================================\")\n",
    "print(\"Training time: \", round(train_end - train_start, 4), \"seconds\")\n",
    "print(\"Labeling time: \", round(label_end - label_start, 4), \"seconds\")\n",
    "print(\"==================================\")\n",
    "print(\"averages of nonfraud averages using training\\n\", train_averages[0])\n",
    "print(\"averages of fraud averages using training data\\n\", train_averages[1])\n",
    "print(\"==================================\")\n",
    "print(\"Train predictions:\\n\", train_pred_using_train_avg[0:20])\n",
    "print(\"Train real:\\n\", training_data_y[0:20])\n",
    "print(\"Test predictions:\\n\", test_pred_using_train_avg[0:20])\n",
    "print(\"Test real:\\n\", test_data_y[0:20])\n",
    "print(\"==================================\")\n",
    "print(\"Train pred unique counts:\\n\", dict(zip(train_unique, train_counts)))\n",
    "print(\"Train real unique counts:\\n\", dict(zip(realTrain_unique, realTrain_counts)))\n",
    "print(\"Test pred unique counts:\\n\", dict(zip(test_unique, test_counts)))\n",
    "print(\"Test real unique counts:\\n\", dict(zip(realTest_unique, realTest_counts)))\n",
    "print(\"==================================\")\n",
    "\n",
    "# TODO: now calculate the accuracy of the classifier using the function implemented in P1, and print it out\n",
    "print(\"Train Fraud averages:\\n\", train_averages[0])\n",
    "print(\"Train Non-fraud averages:\\n\", train_averages[1])\n",
    "print(\"Test Fraud averages:\\n\", test_averages[0])\n",
    "print(\"Test Non-fraud averages:\\n\", test_averages[1])\n",
    "print(\"==================================\")\n",
    "print(f'TRAIN ACCURACY using training averages:\\t{accuracy(training_data_y, train_pred_using_train_avg)}')\n",
    "print(f'TEST ACCURACY using training averages:\\t{accuracy(test_data_y, test_pred_using_train_avg)}')\n",
    "print(f'TRAIN ACCURACY using testing averages:\\t{accuracy(training_data_y, train_pred_using_test_avg)}')\n",
    "print(f'TEST ACCURACY using testing averages:\\t{accuracy(test_data_y, test_pred_using_test_avg)}')\n",
    "print(\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Average accuracy: ~93% Average time: ~0.0635 seconds\n",
    "-----\n",
    "This version works better than the first one. It seems that the features isDeclined, isHighRiskCountry, and checkbook frequency are great at predicting whether a transaction is fraud or not. Speed is the same. In order for the algorithm to be more efficient, I'd have to cut out the classification arrays and find some logarithmic or linear function to cut away the nested for loop. But not bad regardless. 0.0635 seconds for a data set with 2000 entries. So for 1,000,000 entries the time to train would be 31.75 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Implement a logistic regression classifier using sklearn (8 pts)\n",
    "Implement a logistic regression function using the sklearn library. \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the logistic regression here in a function\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def log_regr_withRegularization(train_x, train_y, test_x, test_y):\n",
    "\n",
    "    # Logistic regression with Regularization.\n",
    "    clf = LogisticRegression(max_iter = 800, random_state = 0)\n",
    "    \n",
    "    # Generate the model.\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    # Various data extractions from the model.\n",
    "    predictions = clf.predict(test_x)\n",
    "    parameters = clf.get_params()\n",
    "    probabilities = clf.predict_proba(train_x)\n",
    "    score = clf.score(test_x, test_y)\n",
    "\n",
    "    return predictions, parameters, score, probabilities\n",
    "\n",
    "def log_regr_noRegularization(train_x, train_y, test_x, test_y):\n",
    "\n",
    "    # No regularization.\n",
    "    clf = LogisticRegression(max_iter = 800, penalty = 'none', random_state = 0)\n",
    "    \n",
    "    # Generat the model.\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    # Various data extractions from the model.\n",
    "    predictions = clf.predict(test_x)\n",
    "    parameters = clf.get_params()\n",
    "    score = clf.score(test_x, test_y)\n",
    "    probabilities = clf.predict_proba(train_x)\n",
    "\n",
    "    return predictions, parameters, score, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: now, run some experiments with it, and measure the accuracy with various parametrizations. In particular, you should run it with and without regularization. \n",
    "# In the last line, print the accuracy with the best parameters.\n",
    "\n",
    "import time\n",
    "\n",
    "regr_start = time.time()\n",
    "TrainPred, TrainParam, TrainScore, TrainProb = log_regr_withRegularization(training_data_x, training_data_y, training_data_x, training_data_y)\n",
    "regr_end = time.time()\n",
    "TestPred, TestParam, TestScore, TestProb = log_regr_withRegularization(training_data_x, training_data_y, test_data_x, test_data_y)\n",
    "\n",
    "no_regr_start = time.time()\n",
    "noRegTrainPred, noRegTrainParam, noRegTrainScore, noRegTrainProb = log_regr_noRegularization(training_data_x, training_data_y, training_data_x, training_data_y)\n",
    "no_regr_end = time.time()\n",
    "noRegTestPred, noRegTestParam, noRegTestScore, noRegTestProb = log_regr_noRegularization(training_data_x, training_data_y, test_data_x, test_data_y)\n",
    "\n",
    "print(\"===================\")\n",
    "print(\"No Regularization:\")\n",
    "print(\"===================\")\n",
    "print(\"Training Prediction:\\n\", noRegTrainPred)\n",
    "print(\"Testing Prediction:\\n\", noRegTestPred)\n",
    "print(\"Training Parameters:\\n\", noRegTrainParam)\n",
    "print(\"Testing Parameters:\\n\", noRegTestParam)\n",
    "print(\"Training Score:\\n\", noRegTrainScore)\n",
    "print(\"Testing Score:\\n\", noRegTestScore)\n",
    "print(\"Training Probabilities:\\n\", noRegTrainProb)\n",
    "print(\"Testing Probabilities:\\n\", noRegTestProb)\n",
    "print(\"Training Time:\\n\", no_regr_end - no_regr_start)\n",
    "\n",
    "print(\"===================\")\n",
    "print(\"Regularization:\")\n",
    "print(\"===================\")\n",
    "print(\"Training Prediction:\\n\", TrainPred)\n",
    "print(\"Testing Prediction:\\n\", TestPred)\n",
    "print(\"Training Parameters:\\n\", TrainParam)\n",
    "print(\"Testing Parameters:\\n\", TestParam)\n",
    "print(\"Training Score:\\n\", TrainScore)\n",
    "print(\"Testing Score:\\n\", TestScore)\n",
    "print(\"Training Probabilities:\\n\", TrainProb)\n",
    "print(\"Testing Probabilities:\\n\", TestProb)\n",
    "print(\"Training Time:\\n\", regr_end - regr_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe in one paragraph your experiments and evaluation of the Logistic Regression classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. Compare it with the accuracy of the hand-engineered classifier.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The classifier achieves a higher accuracy than my model. This one gets 99% accuracy while mine has 93%, however this model takes ~.16 seconds on average while mine takes ~.06. So mine is about 3 times faster with 6% less accuracy. If a fraud algorithm that is more efficient than accurate is needed, mine would work better for millions of incoming transactions per second. However, despite my version being more efficient I would always use the scikit learn one because it is highly optimized to work well under different sorts of data while my version works well for a niche data point. Furthermore, my version does also allow parameter tweaking. You can change the majority percentage needed for a data point to be considered fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5 Bonus: Implement a random forest classifier using sklearn (5 pts)\n",
    "Implement a random forest classifier using sklearn \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the random forest classifier here\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def forest(train_x, train_y, test_x, test_y):\n",
    "\n",
    "    # Generate the classifier.\n",
    "    clf = RandomForestClassifier(random_state = 0)\n",
    "    clf.fit(train_x, train_y)\n",
    "    parameters = clf.get_params()\n",
    "    predictions = clf.predict(test_x)\n",
    "    score = clf.score(test_x, test_y)\n",
    "    probabilities = clf.predict_proba(test_x)\n",
    "\n",
    "    return predictions, parameters, score, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform some experiments here with different parameters of the random forest classifier. In the last line, print the accuracy with the best parameters.\n",
    "\n",
    "start = time.time()\n",
    "trainPred, trainParam, trainScore, trainProb = forest(training_data_x, training_data_y, training_data_x, training_data_y)\n",
    "end = time.time()\n",
    "testPred, testParam, testScore, testProb = forest(training_data_x, training_data_y, test_data_x, test_data_y)\n",
    "\n",
    "print(\"===================\")\n",
    "print(\"No Regularization:\")\n",
    "print(\"===================\")\n",
    "print(\"Training Prediction:\\n\", trainPred)\n",
    "print(\"Testing Prediction:\\n\", testPred)\n",
    "print(\"Training Parameters:\\n\", trainParam)\n",
    "print(\"Testing Parameters:\\n\", testParam)\n",
    "print(\"Training Score:\\n\", trainScore)\n",
    "print(\"Testing Score:\\n\", testScore)\n",
    "print(\"Training Probabilities:\\n\", trainProb)\n",
    "print(\"Testing Probabilities:\\n\", testProb)\n",
    "print(\"Training Time:\\n\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe in one paragraph your experiments and evaluation of the random forest classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Both scores were great for classifying both groups. However, it took .32 seconds for only 2000 data points which is concerning because for 10M data points it would take 1600 seconds ~30 minutes. It wouldn't work to classify transactions in real time. Accuracy was high, however I believe the scikit learn logRegression works better to classify data with more efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P6 Bonus: Implement an AdaBoost classifer using sklearn (5 pts)\n",
    "\n",
    "Implement an AdaBoost classifier using sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the adaboost classifier here\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def ada(train_x, train_y, test_x, test_y):\n",
    "\n",
    "    # Generate the classifier.\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(train_x, train_y)\n",
    "    parameters = clf.get_params()\n",
    "    predictions = clf.predict(test_x)\n",
    "    score = clf.score(test_x, test_y)\n",
    "    probabilities = clf.predict_proba(test_x)\n",
    "\n",
    "    return predictions, parameters, score, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform some experiments here with different parametrizations of the adaboost classifier. In the last line, print the accuracy with the best parameters.\n",
    "\n",
    "start = time.time()\n",
    "trainPred, trainParam, trainScore, trainProb = ada(training_data_x, training_data_y, training_data_x, training_data_y)\n",
    "end = time.time()\n",
    "testPred, testParam, testScore, testProb = ada(training_data_x, training_data_y, test_data_x, test_data_y)\n",
    "\n",
    "print(\"===================\")\n",
    "print(\"No Regularization:\")\n",
    "print(\"===================\")\n",
    "print(\"Training Prediction:\\n\", trainPred)\n",
    "print(\"Testing Prediction:\\n\", testPred)\n",
    "print(\"Training Parameters:\\n\", trainParam)\n",
    "print(\"Testing Parameters:\\n\", testParam)\n",
    "print(\"Training Score:\\n\", trainScore)\n",
    "print(\"Testing Score:\\n\", testScore)\n",
    "print(\"Training Probabilities:\\n\", trainProb)\n",
    "print(\"Testing Probabilities:\\n\", testProb)\n",
    "print(\"Training Time:\\n\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe in one paragraph your experiments and evaluation of the AdaBoost classifier. Consider things such as accuracy, training time, ease oftweaking of the parameters.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "With a training time of 0.21 seconds and an accuracy of 99.5% for the train and 98% for the test, this algorithm would likely come in third place between the logistic regression and the random forest algorithm. Tweaking parameters is easy using the scikit API, they have a bunch of different cases for different statistical assumptions on their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Came already in original project.\n",
    "\n",
    "# SOLUTION\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(training_data_x, training_data_y)\n",
    "yhat = clf.predict(test_data_x)\n",
    "acc = accuracy(test_data_y, yhat)\n",
    "print(f\"Accuracy of the AdaBoost classifier {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "13140b3b9092b9a26a4b55ddc500d8b0c9f21b15e8ef2dd16bed19d6074a1e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
