{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Using Deep Convolutional Neural Networks\n",
    "\n",
    "This project requires you to experiment with a deep convolutional network to classify images of squirrels, raccoons and wombats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import datetime\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "# imports for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#import PIL \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "from ImageDataset import DatasetCfg, create_dataset\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up\n",
    "\n",
    "Setting up the directories for the data and temporary files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Code, windows path doesn't work on MacOS (Unix variant)\n",
    "# root_dir = pathlib.WindowsPath(\".\")\n",
    "# root_temp_dir = pathlib.Path(root_dir)\n",
    "# code_dir = pathlib.Path(root_dir)\n",
    "# temp_dir = pathlib.Path(root_temp_dir, \"_Temporary\")\n",
    "# temp_dir.mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# Makes a new pure path for the training to be saved in this \"temporary checkpoints\" directory.\n",
    "root_dir = PurePath(\".\")\n",
    "root_temp_dir = pathlib.Path(root_dir)\n",
    "code_dir = pathlib.Path(root_dir)\n",
    "temp_dir = pathlib.Path(root_temp_dir, \"_Temporary\")\n",
    "temp_dir.mkdir(exist_ok = True, parents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(cfg, modelname):\n",
    "    if modelname == \"simple-with-dropout\":\n",
    "        return create_standard_CNN(cfg, 0.2)\n",
    "    if modelname == \"simple-no-dropout\":\n",
    "        return create_standard_CNN(cfg, 0.0)\n",
    "    if modelname == \"inception\":\n",
    "        return create_inception(cfg)\n",
    "    if modelname == \"my-network-1\":\n",
    "        return create_my_network_1(cfg)\n",
    "    if modelname == \"my-network-2\":\n",
    "        return create_my_network_2(cfg)\n",
    "    if modelname == \"my-network-3\":\n",
    "        return create_my_network_3(cfg)\n",
    "    raise RuntimeError(f\"unknown model {modelname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_CNN(cfg, dropout = 0.2, activation = 'relu'):\n",
    "    \"\"\" Create a simple model with dropout\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(16, 3, padding='same', activation=activation, input_shape=(cfg.IMG_HEIGHT, cfg.IMG_WIDTH ,3)),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(dropout),\n",
    "        Conv2D(32, 3, padding='same', activation=activation),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, 3, padding='same', activation=activation),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(len(cfg.CLASS_NAMES)) \n",
    "    ])\n",
    "    # \n",
    "    # in this case, it does not have a softmax output... because we are using cross-validation\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inception(cfg):\n",
    "    \"\"\" Create a model starting from the feature extraction layers of an Inception v3 network, \n",
    "    pre-trained on ImageNet. Train only the classifier on top of this network. \n",
    "    \"\"\"\n",
    "    # https://keras.io/api/applications/\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer -- let's say we have 200 classes\n",
    "    #predictions = Dense(200, activation='softmax')(x)\n",
    "    predictions = Dense(3)(x)\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_my_network_1(cfg):\n",
    "    \"\"\"Replace this model with an alternative model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(cfg.IMG_HEIGHT, cfg.IMG_WIDTH ,3)));\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(len(cfg.CLASS_NAMES))) \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_my_network_2(cfg):\n",
    "    \"\"\"Replace this model with an alternative model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(cfg.IMG_HEIGHT, cfg.IMG_WIDTH ,3)));\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(len(cfg.CLASS_NAMES))) \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_my_network_3(cfg):\n",
    "    \"\"\"Replace this model with an alternative model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(cfg.IMG_HEIGHT, cfg.IMG_WIDTH ,3)));\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(len(cfg.CLASS_NAMES))) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentSpecification:\n",
    "    \"\"\"Contains the various parameters of the experiment\"\"\"\n",
    "    def __init__(self, datasetname = None, modelname = None):\n",
    "        self.modelname = modelname\n",
    "        self.datasetname = datasetname\n",
    "        self.optimizer = \"adam\"\n",
    "        self.target_epochs = 5\n",
    "        \n",
    "    def get_expname(self):\n",
    "        \"\"\"Returns a name generated from the parameters that can distinguish between \n",
    "        the experiments. The idea is that the different experiments will be put into \n",
    "        different directories\"\"\"\n",
    "        return self.modelname + \"-\" + self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentResults:\n",
    "    \"\"\" The class which will be saved and contains the experiment results \"\"\"\n",
    "    def __init__(self, exp_spec):\n",
    "        self.exp_spec = exp_spec\n",
    "        self.expname = exp_spec.get_expname()\n",
    "        self.modelname = exp_spec.modelname\n",
    "        self.epochs_trained = 0\n",
    "        self.metrics = {'loss' : [], 'accuracy': [], 'val_loss': [], 'val_accuracy' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_experiment(exp_spec):\n",
    "    \"\"\" Runs a training experiment, returns the experimental results. The same function can be used \n",
    "    to return the experimental result\"\"\"\n",
    "    # the directory of the current setup in temp\n",
    "    temp_model_dir = pathlib.Path(temp_dir, exp_spec.get_expname())\n",
    "    temp_model_dir.mkdir(exist_ok = True, parents = True)\n",
    "    # paths to various files\n",
    "    er_path = pathlib.Path(temp_model_dir, exp_spec.get_expname() + \"_exprecord.pkl\")\n",
    "    checkpoint_path = pathlib.Path(temp_model_dir, \"checkpoint.ckpt\")\n",
    "    \n",
    "    \n",
    "    # HERE\n",
    "    exp_spec.dataset_dir = PurePath(root_dir, exp_spec.datasetname)\n",
    "#     if not exp_spec.dataset_dir.exists():\n",
    "#         raise RuntimeError(f\"Dataset directory does not exist {exp_spec.dataset_dir}\")\n",
    "    \n",
    "    print(exp_spec.dataset_dir)\n",
    "    \n",
    "    if not os.path.isdir(exp_spec.dataset_dir):\n",
    "        raise RuntimeError(f\"Dataset directory does not exist {exp_spec.dataset_dir}\")\n",
    "    \n",
    "    cfg = DatasetCfg(path_to_data = exp_spec.dataset_dir)\n",
    "    \n",
    "    # Load or create the experiment results. If we are done, just return that.\n",
    "    if er_path.exists():\n",
    "        with open(er_path, 'rb') as f:\n",
    "            er = pickle.load(f)\n",
    "        print(f\"> > > Loaded the previous experiment results successfully from {er_path}\")\n",
    "        if er.epochs_trained >= exp_spec.target_epochs:\n",
    "            print(f\"> > > The experiment results have training epochs {er.epochs_trained},\"\n",
    "                  f\" which is more than the one requested {exp_spec.target_epochs}, returning it without further training.\")\n",
    "            return er\n",
    "    else:\n",
    "        er = ExperimentResults(exp_spec)\n",
    "    \n",
    "    model = create_model(cfg, exp_spec.modelname)    \n",
    "    # if there is something to load, load it back\n",
    "    if er.epochs_trained > 0:\n",
    "        model.load_weights(str(checkpoint_path))\n",
    "    model.compile(optimizer=exp_spec.optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    # callback for saving the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=str(checkpoint_path),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    # create the training data\n",
    "    train_data_gen, validation_data_gen = create_dataset(cfg)\n",
    "\n",
    "    # proceeding in steps of only one epoch at a time, save the results etc.\n",
    "    while er.epochs_trained < exp_spec.target_epochs:\n",
    "        print(f\"> > > Training {exp_spec.get_expname()} epoch {er.epochs_trained + 1} out of {exp_spec.target_epochs}\")\n",
    "        history = model.fit(\n",
    "            train_data_gen,\n",
    "            steps_per_epoch= cfg.training_count // cfg.BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            validation_data=validation_data_gen,\n",
    "            validation_steps= cfg.validation_count // cfg.BATCH_SIZE,\n",
    "            callbacks=[cp_callback]\n",
    "        )\n",
    "        # copy the data\n",
    "        er.epochs_trained = er.epochs_trained + 1\n",
    "        for metric in ['loss', 'accuracy', 'val_loss', 'val_accuracy']:\n",
    "            er.metrics[metric].append(history.history[metric][0])\n",
    "        # update the accuracy etc here\n",
    "        with open(er_path, 'wb') as f:\n",
    "            # Pickle the experiment results\n",
    "            pickle.dump(er, f)\n",
    "    print(f\"> > > Training of the requested number of epochs {exp_spec.target_epochs} done successfully.\")\n",
    "    return er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modelname': 'simple-with-dropout', 'datasetname': '.', 'optimizer': 'adam', 'target_epochs': 5}\n",
      ".\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "cannot instantiate 'WindowsPath' on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-363-d2d09478eeda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mexp_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-362-f841dd64d4b4>\u001b[0m in \u001b[0;36mrun_training_experiment\u001b[0;34m(exp_spec)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset directory does not exist {exp_spec.dataset_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetCfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Load or create the experiment results. If we are done, just return that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ml3/P3-RaccoonSquirrelWombat/ImageDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_data)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# training data is in a dir called training_data, each subdir is a class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*/*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m             raise NotImplementedError(\"cannot instantiate %r on your system\"\n\u001b[0m\u001b[1;32m   1041\u001b[0m                                       % (cls.__name__,))\n\u001b[1;32m   1042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: cannot instantiate 'WindowsPath' on your system"
     ]
    }
   ],
   "source": [
    "# Run all the specified experiments \n",
    "ds_name = \".\"\n",
    "modelnames = [\"simple-with-dropout\", \"simple-no-dropout\", \"inception\", \"my-network-1\", \"my-network-2\", \"my-network-3\"]\n",
    "ers = dict()\n",
    "\n",
    "for modelname in modelnames:\n",
    "    exp_spec = ExperimentSpecification(datasetname = ds_name, modelname = modelname)\n",
    "    print(exp_spec.__dict__)\n",
    "    exp_spec.target_epochs = 12\n",
    "    er = run_training_experiment(exp_spec)\n",
    "    ers[modelname] = er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.subplot(2,3,i+1)\n",
    "def plot_experiment_results(er):\n",
    "    #plt.title(er.exp_spec.get_expname())\n",
    "    plt.plot(er.metrics['loss'])\n",
    "    plt.plot(er.metrics['val_loss'])\n",
    "    \n",
    "def plot_metric(metric, ers):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(metric)\n",
    "    for name in ers:\n",
    "        plt.plot(ers[name].metrics[metric], label=name)\n",
    "    ax.legend()\n",
    "    \n",
    "#  for name in ers:\n",
    "#    plot_experiment_results(ers[name])\n",
    "\n",
    "plot_metric('loss', ers)\n",
    "plot_metric('val_loss', ers)\n",
    "plot_metric('accuracy', ers)\n",
    "plot_metric('val_accuracy', ers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "13140b3b9092b9a26a4b55ddc500d8b0c9f21b15e8ef2dd16bed19d6074a1e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
